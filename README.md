# Speaker Identification System
# ÿ™ÿ¥ÿÆ€åÿµ ŸáŸà€åÿ™ ⁄ØŸà€åŸÜÿØŸá ÿßÿ≤ ÿ±Ÿà€å ÿµÿØÿß

**A comprehensive speaker identification system with three parallel model approaches**

## üìã Overview

ÿß€åŸÜ Ÿæÿ±Ÿà⁄òŸá €å⁄© ÿ≥€åÿ≥ÿ™ŸÖ ⁄©ÿßŸÖŸÑ ÿ™ÿ¥ÿÆ€åÿµ ŸáŸà€åÿ™ ⁄ØŸà€åŸÜÿØŸá ÿ®ÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ ÿ≥Ÿá ÿ±Ÿàÿ¥ ŸÖÿÆÿ™ŸÑŸÅ ÿßÿ≥ÿ™:

1. **Classic Model**: MFCC features + SVM classifier
2. **Semi-Professional Model**: ECAPA-TDNN deep learning embeddings  
3. **Deep Dual Model**: ECAPA-TDNN + X-Vector (parallel deep models)

## üéØ Features

- ‚úÖ Live audio streaming from microphone (16kHz, mono)
- ‚úÖ 5-second windowed processing
- ‚úÖ Real-time speaker identification
- ‚úÖ Mel spectrogram visualization
- ‚úÖ Speaker registration interface
- ‚úÖ GPU acceleration (with CPU fallback)
- ‚úÖ Three parallel identification approaches
- ‚úÖ Confidence scoring for all models

## üèóÔ∏è Project Structure

```
SedaGozar/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ audio/              # Audio capture and recording
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stream.py       # Live streaming with 5-sec buffers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recorder.py     # Recording for registration
‚îÇ   ‚îú‚îÄ‚îÄ features/           # Feature extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mfcc.py         # MFCC extraction (40-dim)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spectrogram.py  # Mel spectrogram visualization
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Speaker identification models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classic.py      # MFCC + SVM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semipro.py      # ECAPA-TDNN embeddings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deep.py         # Dual deep models
‚îÇ   ‚îú‚îÄ‚îÄ database/           # Speaker data management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manager.py      # Database operations
‚îÇ   ‚îî‚îÄ‚îÄ ui/                 # User interface
‚îÇ       ‚îî‚îÄ‚îÄ gradio_app.py   # Gradio web interface
‚îú‚îÄ‚îÄ data/                   # Speaker database
‚îÇ   ‚îú‚îÄ‚îÄ audio/              # Speaker audio samples
‚îÇ   ‚îú‚îÄ‚îÄ features/           # MFCC features
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/         # Deep learning embeddings
‚îú‚îÄ‚îÄ main.py                 # Application entry point
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îî‚îÄ‚îÄ README.md               # This file
```

## üöÄ Installation

### Prerequisites

- Python 3.8 or higher
- Microphone access
- (Optional) NVIDIA GPU with CUDA for faster processing

### Step 1: Clone/Download the Project

```bash
cd c:\Users\Mehdi\PycharmProjects\SedaGozar
```

### Step 2: Create Virtual Environment (Recommended)

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Note for Windows users**: If PyAudio installation fails, download the wheel from:
https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio

Then install with:
```bash
pip install PyAudio‚Äë0.2.XX‚ÄëcpXX‚ÄëcpXX‚Äëwin_amd64.whl
```

## üíª Usage

### Running the Application

```bash
python main.py
```

The application will start and open at: **http://localhost:7860**

### Tab 1: Live Speaker Identification

1. Click **"Start Recording"**
2. Speak into the microphone
3. Every 5 seconds:
   - System processes audio
   - Updates spectrogram visualization
   - Shows predictions from all three models
4. Click **"Get Latest Results"** to see current predictions
5. Click **"Stop Recording"** when done

### Tab 2: Speaker Registration

1. Set recording duration (3-10 seconds, recommended: 5 seconds)
2. Click **"Record Sample"**
3. Speak naturally (different sentences, not the same phrase)
4. Enter the speaker's name
5. Click **"Save Speaker"**
6. System automatically trains all models with new speaker

## üìä Model Comparison

### Classic Model (MFCC + SVM)

**Approach**: Extracts 40 MFCC coefficients, computes mean and variance (80 features), trains SVM classifier

**Strengths**:
- Fast (no GPU needed)
- Works with small datasets (2-5 speakers)
- Interpretable features
- Low memory footprint

**Weaknesses**:
- Sensitive to background noise
- Microphone variation affects accuracy
- Limited robustness

**Confidence Score**: 0-100% probability from SVM

### Semi-Professional Model (ECAPA-TDNN)

**Approach**: Uses pretrained ECAPA-TDNN model to extract 192-dim embeddings, identifies via cosine similarity

**Strengths**:
- Much more robust to noise
- Pretrained on 1000s of hours (VoxCeleb)
- Better generalization
- No feature engineering needed

**Weaknesses**:
- Requires GPU for real-time performance (works on CPU)
- Less interpretable
- Domain mismatch possible

**Similarity Score**: 0-100% (cosine similarity normalized)

### Deep Dual Model (ECAPA + X-Vector)

**Approach**: Runs TWO deep models in parallel (ECAPA-TDNN + X-Vector), shows independent predictions

**Strengths**:
- Highest accuracy
- Maximum robustness
- Redundancy (if one fails, other may succeed)
- Can detect uncertain predictions (disagreement)

**Weaknesses**:
- 2x computational cost
- 2x memory requirement
- Slower inference
- Overkill for small speaker sets

**Scores**: Two independent scores (0-100%) from each model

## üéØ Confidence Score Interpretation

| Range | Meaning | Action |
|-------|---------|--------|
| >85% | Very high confidence | Strongly believe identification is correct ‚úÖ |
| 70-85% | High confidence | Likely correct ‚úÖ |
| 60-70% | Moderate confidence | Uncertain ‚ö†Ô∏è |
| <60% | Low confidence | Unreliable, possibly unknown speaker ‚ùå |

## ‚ö†Ô∏è Limitations

### 1. **Noise Sensitivity**
   - Classic model: Most sensitive
   - Semi-pro model: More robust
   - Deep models: Most robust
   - Recommendation: Record in quiet environment

### 2. **Microphone Variation**
   - Different microphones between registration and identification reduce accuracy
   - Recommendation: Use same microphone for consistency

### 3. **Speaker Health/Emotion**
   - Voice changes due to illness, stress, or emotion affect performance
   - Recommendation: Re-register if voice characteristics change significantly

### 4. **Short Utterances**
   - System needs ~2-3 seconds of actual speech in 5-second buffer
   - Silence or very short speech produces unreliable results
   - Recommendation: Speak continuously during recording

### 5. **Number of Speakers**
   - Minimum: 2 speakers
   - Optimal: 3-20 speakers
   - More speakers increase confusion between similar voices

## üîß Troubleshooting

### PyAudio Installation Issues

**Windows**: Download wheel from https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio

**Linux**: 
```bash
sudo apt-get install portaudio19-dev
pip install pyaudio
```

**Mac**:
```bash
brew install portaudio
pip install pyaudio
```

### GPU Not Detected

Check CUDA installation:
```python
import torch
print(torch.cuda.is_available())
```

If False but GPU exists, reinstall PyTorch with CUDA support:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Models Not Loading

Ensure SpeechBrain is installed:
```bash
pip install speechbrain
```

First run will download pretrained models (~500MB), this is normal.

## üìñ Technical Details

### Feature Extraction

**MFCC (Mel-Frequency Cepstral Coefficients)**:
- 40 coefficients extracted using librosa
- Mean and variance computed over time ‚Üí 80-dimensional vector
- Captures spectral envelope (vocal tract characteristics)

**Mel Spectrogram**:
- 128 Mel bands
- Time-frequency representation
- Visualizes audio content

### Models Architecture

**ECAPA-TDNN**:
- Emphasized Channel Attention
- Res2Net backbone
- Temporal pooling
- 192-dim embeddings

**X-Vector**:
- Frame-level TDNN
- Statistics pooling
- Speaker embeddings
- 512-dim (in SpeechBrain)

### Similarity Metric

Cosine similarity between normalized embeddings:
```
similarity = (emb1 ¬∑ emb2) / (||emb1|| * ||emb2||)
```

Range: [-1, 1], converted to [0, 100]%

## üìö Scientific Background

This project implements multiple speaker recognition paradigms:

1. **Traditional ML**: Feature engineering (MFCCs) + discriminative classifier (SVM)
2. **Deep Learning**: End-to-end learned embeddings (ECAPA-TDNN)
3. **Ensemble**: Multiple model consensus for robustness

**Key Papers**:
- ECAPA-TDNN: "ECAPA-TDNN: Emphasized Channel Attention..." (Interspeech 2020)
- X-Vector: "X-Vectors: Robust DNN Embeddings..." (ICASSP 2018)
- MFCC: Classic speech processing (Davis & Mermelstein, 1980)

## üéì Academic Use

This project is suitable for university presentations/reports. Key academic aspects:

- ‚úÖ Multiple approaches comparison (classic vs deep learning)
- ‚úÖ Detailed scientific rationale in code comments
- ‚úÖ Explainable confidence scores
- ‚úÖ Documented limitations and failure cases
- ‚úÖ Real-world applicability demonstration

## ü§ù Contributing

ÿß€åŸÜ Ÿæÿ±Ÿà⁄òŸá ÿ®ÿ±ÿß€å ÿßŸáÿØÿßŸÅ ÿ¢ŸÖŸàÿ≤ÿ¥€å Ÿà ÿ™ÿ≠ŸÇ€åŸÇÿßÿ™€å ÿ∑ÿ±ÿßÿ≠€å ÿ¥ÿØŸá ÿßÿ≥ÿ™.

## üìÑ License

Educational/Research use only.

## üë®‚Äçüíª Development

### Adding New Models

1. Create new file in `src/models/`
2. Implement `predict()` method
3. Add to `gradio_app.py` interface
4. Update README

### Adding New Features

- Feature extractors go in `src/features/`
- Follow existing pattern with caching
- Document scientific rationale

## üîó Resources

- SpeechBrain: https://speechbrain.github.io/
- VoxCeleb Dataset: https://www.robots.ox.ac.uk/~vgg/data/voxceleb/
- Librosa Documentation: https://librosa.org/

---

**Made with ‚ù§Ô∏è for Speaker Recognition Research**
